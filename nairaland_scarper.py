# -*- coding: utf-8 -*-
"""Nairaland_scarper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UXxE4eXsQHcLUbPX2tdUPqQXft2Uoc-6
"""

import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import time
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import os

# Set page configuration
st.set_page_config(
    page_title="Nairaland Profile Analyzer",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Add custom CSS for styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #2e7d32;
        text-align: center;
        margin-bottom: 20px;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #4caf50;
        margin-bottom: 10px;
    }
    .insight-box {
        background-color: #f1f8e9;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 10px;
    }
    .stProgress > div > div > div > div {
        background-color: #4caf50;
    }
    .metric-card {
        background-color: #e8f5e9;
        border-radius: 5px;
        padding: 15px;
        text-align: center;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .metric-value {
        font-size: 1.8rem;
        font-weight: bold;
        color: #2e7d32;
    }
    .metric-label {
        font-size: 1rem;
        color: #555;
    }
</style>
""", unsafe_allow_html=True)

# Main header
st.markdown("<h1 class='main-header'>Nairaland Profile Analyzer</h1>", unsafe_allow_html=True)

# URLs of Nairaland profiles
default_urls = [
    "https://www.nairaland.com/elusive001",
    "https://www.nairaland.com/botragelad",
    "https://www.nairaland.com/holiness2100",
    "https://www.nairaland.com/uprightness100",
    "https://www.nairaland.com/truthU87",
    "https://www.nairaland.com/biodun556",
    "https://www.nairaland.com/coronaVirusPro",
    "https://www.nairaland.com/NigerianXXX",
    "https://www.nairaland.com/Kingsnairaland",
    "https://www.nairaland.com/Betscoreodds",
    "https://www.nairaland.com/Nancy2020",
    "https://www.nairaland.com/Nancy1986",
    "https://www.nairaland.com/Writernig",
    "https://www.nairaland.com/WritterNg",
    "https://www.nairaland.com/WriiterNg",
    "https://www.nairaland.com/WrriterNg",
    "https://www.nairaland.com/WriteerNig",
    "https://www.nairaland.com/WriterrNig",
    "https://www.nairaland.com/WritterNig",
    "https://www.nairaland.com/WriiterNig",
    "https://www.nairaland.com/WrriterNig",
    "https://www.nairaland.com/WriterNigg",
    "https://www.nairaland.com/WriterNiiig",
    "https://www.nairaland.com/WriterNiig",
    "https://www.nairaland.com/Ken6488",
    "https://www.nairaland.com/Dalil8",
    "https://www.nairaland.com/Slavaukraini",
    "https://www.nairaland.com/Redscorpion",
    "https://www.nairaland.com/Nigeriazoo"
]

# Sidebar configuration
with st.sidebar:
    st.header("Configuration")
    
    # Option to use default URLs or enter custom ones
    use_default = st.checkbox("Use default URLs", value=True)
    
    if not use_default:
        custom_urls = st.text_area("Enter Nairaland profile URLs (one per line)", height=200)
        urls = [url.strip() for url in custom_urls.split('\n') if url.strip()]
    else:
        urls = default_urls
    
    # Scraping options
    st.subheader("Scraping Options")
    max_posts = st.slider("Maximum posts to scrape per profile", 10, 500, 100, 10)
    use_selenium = st.checkbox("Use Selenium (recommended for avoiding Cloudflare)", value=True)
    
    # Analysis options
    st.subheader("Analysis Options")
    time_window = st.selectbox("Time window for coordination analysis", 
                             ["1 hour", "3 hours", "6 hours", "12 hours", "1 day", "1 week"], 
                             index=2)
    
    # Start scraping
    start_scrape = st.button("Start Scraping", type="primary")
    
    # About section
    st.markdown("---")
    st.markdown("### About")
    st.info("""
    This dashboard scrapes Nairaland user profiles and analyzes potential coordination patterns between accounts.
    
    Data collected includes:
    - Username
    - Post content
    - Publication date
    - Replies
    - Registration date
    """)

# Function to set up Selenium WebDriver
@st.cache_resource
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    
    # Add user-agent to avoid detection
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")
    
    # Install ChromeDriver using webdriver_manager
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        return driver
    except Exception as e:
        st.error(f"Failed to initialize Chrome WebDriver: {e}")
        # Fallback to explicitly checking for ChromeDriver
        if os.path.exists("/usr/bin/chromedriver"):
            driver = webdriver.Chrome(service=Service("/usr/bin/chromedriver"), options=options)
            return driver
        else:
            st.error("ChromeDriver not found in the system. Please make sure Chrome and ChromeDriver are installed.")
            return None

# Function to get profile info using Selenium
def get_profile_data_selenium(url, max_posts=100):
    driver = get_driver()
    if not driver:
        return pd.DataFrame()
    
    try:
        driver.get(url)
        
        # Wait for Cloudflare to clear
        time.sleep(5)
        
        # Extract username from URL
        username = url.split('/')[-1]
        
        # Try to find registration date
        try:
            profile_info = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, "//td[contains(text(), 'Joined')]"))
            )
            reg_date_text = profile_info.text
            reg_date_match = re.search(r'Joined: (.+?)(?:\(|\n|$)', reg_date_text)
            registration_date = reg_date_match.group(1).strip() if reg_date_match else "Unknown"
        except:
            registration_date = "Unknown"
        
        # Get user's posts
        posts_data = []
        current_page = 1
        post_count = 0
        
        while post_count < max_posts:
            # Posts are typically in table rows with specific IDs
            try:
                post_elements = WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.XPATH, "//td[contains(@id, 'pb')]"))
                )
                
                date_elements = driver.find_elements(By.XPATH, "//td[contains(@id, 'pb')]/preceding-sibling::td")
                
                for i, (post_element, date_element) in enumerate(zip(post_elements, date_elements)):
                    if post_count >= max_posts:
                        break
                    
                    # Extract post content
                    post_content = post_element.text
                    
                    # Extract publication date
                    date_text = date_element.text
                    pub_date_match = re.search(r'(\d+:\d+[ap]m) On ([a-zA-Z]+ \d+), (\d{4})', date_text)
                    publication_date = f"{pub_date_match.group(1)} {pub_date_match.group(2)}, {pub_date_match.group(3)}" if pub_date_match else "Unknown"
                    
                    # Extract replies (likes) if available
                    likes_match = re.search(r'(\d+) Likes?', post_content)
                    replies = int(likes_match.group(1)) if likes_match else 0
                    
                    # Add to posts data
                    posts_data.append({
                        'Username': username,
                        'Post': post_content,
                        'Publication_Date': publication_date,
                        'Replies': replies,
                        'Registration_Date': registration_date,
                        'Post_ID': f"{username}_{post_count}"
                    })
                    
                    post_count += 1
                
                # Check if there's a next page
                try:
                    next_button = driver.find_element(By.XPATH, f"//a[text()='{current_page + 1}']")
                    next_button.click()
                    current_page += 1
                    time.sleep(2)  # Wait for page to load
                except:
                    break  # No more pages
                    
            except Exception as e:
                st.warning(f"Error scraping posts for {username}: {e}")
                break
                
        return pd.DataFrame(posts_data)
    
    except Exception as e:
        st.error(f"Error scraping profile {url}: {e}")
        return pd.DataFrame()
    
    finally:
        driver.quit()

# Function to get profile info using Requests/BeautifulSoup (fallback)
def get_profile_data_bs4(url, max_posts=100):
    try:
        # Extract username from URL
        username = url.split('/')[-1]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://www.nairaland.com/',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
        }
        
        # Get the profile page
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Try to find registration date
        try:
            profile_info = soup.find('td', text=re.compile('Joined'))
            reg_date_text = profile_info.text if profile_info else "Unknown"
            reg_date_match = re.search(r'Joined: (.+?)(?:\(|\n|$)', reg_date_text)
            registration_date = reg_date_match.group(1).strip() if reg_date_match else "Unknown"
        except:
            registration_date = "Unknown"
        
        # Get user's posts
        posts_data = []
        current_page = 1
        post_count = 0
        
        while post_count < max_posts:
            page_url = url if current_page == 1 else f"{url}/{current_page}"
            page_response = requests.get(page_url, headers=headers, timeout=15)
            page_soup = BeautifulSoup(page_response.text, 'html.parser')
            
            # Posts are typically in table cells with id starting with 'pb'
            post_elements = page_soup.find_all('td', id=re.compile('^pb'))
            
            if not post_elements:
                break
                
            for post_element in post_elements:
                if post_count >= max_posts:
                    break
                
                # Extract post content
                post_content = post_element.text
                
                # Extract publication date
                try:
                    date_element = post_element.find_previous_sibling('td')
                    date_text = date_element.text if date_element else "Unknown"
                    pub_date_match = re.search(r'(\d+:\d+[ap]m) On ([a-zA-Z]+ \d+), (\d{4})', date_text)
                    publication_date = f"{pub_date_match.group(1)} {pub_date_match.group(2)}, {pub_date_match.group(3)}" if pub_date_match else "Unknown"
                except:
                    publication_date = "Unknown"
                
                # Extract replies (likes) if available
                likes_match = re.search(r'(\d+) Likes?', post_content)
                replies = int(likes_match.group(1)) if likes_match else 0
                
                # Add to posts data
                posts_data.append({
                    'Username': username,
                    'Post': post_content,
                    'Publication_Date': publication_date,
                    'Replies': replies,
                    'Registration_Date': registration_date,
                    'Post_ID': f"{username}_{post_count}"
                })
                
                post_count += 1
            
            # Check if there's a next page by looking for a link with the next page number
            next_page_link = page_soup.find('a', text=str(current_page + 1))
            if next_page_link:
                current_page += 1
                time.sleep(1)  # Be nice to the server
            else:
                break
                
        return pd.DataFrame(posts_data)
    
    except Exception as e:
        st.error(f"Error scraping profile {url}: {e}")
        return pd.DataFrame()

# Function to parse dates uniformly
def parse_date(date_str):
    if not date_str or date_str == "Unknown":
        return None
    
    try:
        # Various date formats used on Nairaland
        formats = [
            "%I:%M%p On %b %d, %Y",  # 12:34pm On Jan 01, 2023
            "%I:%M%p On %B %d, %Y",  # 12:34pm On January 01, 2023
            "%d-%b-%y",              # 01-Jan-23
            "%B %d, %Y",             # January 01, 2023
            "%b %d, %Y"              # Jan 01, 2023
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except:
                continue
        
        return None
    except:
        return None

# Function to analyze coordination patterns
def analyze_coordination(df, time_window_str="6 hours"):
    if df.empty:
        return {
            "coordination_pairs": pd.DataFrame(),
            "time_clusters": pd.DataFrame(),
            "similar_content": pd.DataFrame(),
            "post_timeline": pd.DataFrame(),
            "user_activity": pd.DataFrame()
        }
    
    # Convert time window string to hours
    time_window_dict = {
        "1 hour": 1,
        "3 hours": 3,
        "6 hours": 6,
        "12 hours": 12,
        "1 day": 24,
        "1 week": 168
    }
    time_window_hours = time_window_dict.get(time_window_str, 6)
    
    # Process publication dates
    df['Datetime'] = df['Publication_Date'].apply(parse_date)
    
    # Remove entries with invalid dates
    df_valid = df.dropna(subset=['Datetime'])
    
    if df_valid.empty:
        return {
            "coordination_pairs": pd.DataFrame(),
            "time_clusters": pd.DataFrame(),
            "similar_content": pd.DataFrame(),
            "post_timeline": pd.DataFrame(),
            "user_activity": pd.DataFrame()
        }
    
    # 1. Find users posting within the same time window
    coordination_pairs = []
    
    # Create a sorted list of posts by time
    posts_by_time = df_valid.sort_values('Datetime')
    
    # Find posts within time window of each other
    for i, row1 in posts_by_time.iterrows():
        window_end = row1['Datetime'] + pd.Timedelta(hours=time_window_hours)
        window_posts = posts_by_time[
            (posts_by_time['Datetime'] >= row1['Datetime']) & 
            (posts_by_time['Datetime'] <= window_end) &
            (posts_by_time['Username'] != row1['Username'])
        ]
        
        for j, row2 in window_posts.iterrows():
            time_diff = (row2['Datetime'] - row1['Datetime']).total_seconds() / 3600  # in hours
            coordination_pairs.append({
                'User1': row1['Username'],
                'User2': row2['Username'],
                'Post1_ID': row1['Post_ID'],
                'Post2_ID': row2['Post_ID'],
                'Time_Difference_Hours': time_diff,
                'Date1': row1['Datetime'],
                'Date2': row2['Datetime']
            })
    
    coordination_df = pd.DataFrame(coordination_pairs)
    
    # 2. Identify time clusters (times when multiple users post)
    if not coordination_df.empty:
        # Bin all posts by hour to find clusters
        df_valid['Hour'] = df_valid['Datetime'].dt.floor('H')
        time_clusters = df_valid.groupby('Hour')['Username'].nunique().reset_index()
        time_clusters.columns = ['Hour', 'Unique_Users']
        time_clusters = time_clusters.sort_values('Unique_Users', ascending=False)
    else:
        time_clusters = pd.DataFrame(columns=['Hour', 'Unique_Users'])
    
    # 3. Check for similar content patterns
    # Simple similarity - check for same words or patterns
    def extract_keywords(text):
        if not isinstance(text, str):
            return []
        # Simple keyword extraction
        text = text.lower()
        # Remove common words
        stopwords = ['the', 'and', 'is', 'in', 'to', 'a', 'of', 'for', 'on', 'with', 'as', 'this', 'that']
        words = re.findall(r'\b\w+\b', text)
        return [w for w in words if len(w) > 3 and w not in stopwords]
    
    df_valid['Keywords'] = df_valid['Post'].apply(extract_keywords)
    
    similar_content = []
    users = df_valid['Username'].unique()
    
    for i in range(len(users)):
        for j in range(i+1, len(users)):
            user1, user2 = users[i], users[j]
            
            # Get posts from both users
            posts1 = df_valid[df_valid['Username'] == user1]['Keywords'].tolist()
            posts2 = df_valid[df_valid['Username'] == user2]['Keywords'].tolist()
            
            # Flatten lists
            keywords1 = [k for sublist in posts1 for k in sublist]
            keywords2 = [k for sublist in posts2 for k in sublist]
            
            if not keywords1 or not keywords2:
                continue
            
            # Find common keywords
            common = set(keywords1) & set(keywords2)
            
            if common:
                similarity = len(common) / min(len(set(keywords1)), len(set(keywords2)))
                similar_content.append({
                    'User1': user1,
                    'User2': user2,
                    'Common_Keywords': list(common),
                    'Similarity_Score': similarity
                })
    
    # 4. Create post timeline for visualization
    post_timeline = df_valid[['Username', 'Datetime', 'Post_ID']].copy()
    
    # 5. User activity patterns
    user_activity = df_valid.groupby('Username').agg({
        'Post_ID': 'count',
        'Datetime': ['min', 'max']
    }).reset_index()
    
    user_activity.columns = ['Username', 'Post_Count', 'First_Post', 'Last_Post']
    user_activity['Active_Days'] = (user_activity['Last_Post'] - user_activity['First_Post']).dt.total_seconds() / (60*60*24)
    user_activity['Posts_Per_Day'] = user_activity['Post_Count'] / user_activity['Active_Days'].clip(lower=1)  # Avoid division by zero
    
    return {
        "coordination_pairs": pd.DataFrame(coordination_pairs) if coordination_pairs else pd.DataFrame(),
        "time_clusters": time_clusters,
        "similar_content": pd.DataFrame(similar_content) if similar_content else pd.DataFrame(),
        "post_timeline": post_timeline,
        "user_activity": user_activity
    }

# Main scraping and analysis function
def scrape_and_analyze(urls, max_posts, use_selenium, time_window):
    start_time = time.time()
    all_data = pd.DataFrame()
    
    # Create a progress bar
    progress_bar = st.progress(0)
    scraping_status = st.empty()
    
    scraping_status.info("Setting up scraper...")
    
    # Process each URL
    for i, url in enumerate(urls):
        user = url.split('/')[-1]
        scraping_status.info(f"Scraping {user} ({i+1}/{len(urls)})...")
        
        if use_selenium:
            df = get_profile_data_selenium(url, max_posts)
        else:
            df = get_profile_data_bs4(url, max_posts)
        
        all_data = pd.concat([all_data, df], ignore_index=True)
        
        # Update progress
        progress_bar.progress((i + 1) / len(urls))
    
    # Complete progress
    scraping_status.success(f"Scraping completed for {len(urls)} profiles in {time.time() - start_time:.2f} seconds!")
    
    # Analyze the data for coordination patterns
    scraping_status.info("Analyzing coordination patterns...")
    analysis_results = analyze_coordination(all_data, time_window)
    
    scraping_status.success("Analysis complete!")
    
    return all_data, analysis_results

# Main dashboard layout
if start_scrape:
    with st.spinner('Scraping and analyzing profiles... This may take a while.'):
        # Scrape and analyze
        data, analysis = scrape_and_analyze(urls, max_posts, use_selenium, time_window)
        
        # Cache the results
        st.session_state['data'] = data
        st.session_state['analysis'] = analysis
        st.session_state['scraped'] = True

# Display results if data is available
if 'scraped' in st.session_state and st.session_state['scraped']:
    data = st.session_state['data']
    analysis = st.session_state['analysis']
    
    # Check if any data was scraped
    if data.empty:
        st.error("No data was scraped. Please check the URLs and try again.")
    else:
        # Display overview metrics
        st.markdown("<h2 class='sub-header'>Overview</h2>", unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class='metric-card'>
                <div class='metric-value'>{data['Username'].nunique()}</div>
                <div class='metric-label'>Users Scraped</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col2:
            st.markdown(f"""
            <div class='metric-card'>
                <div class='metric-value'>{len(data)}</div>
                <div class='metric-label'>Total Posts</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col3:
            avg_posts = data.groupby('Username').size().mean()
            st.markdown(f"""
            <div class='metric-card'>
                <div class='metric-value'>{avg_posts:.1f}</div>
                <div class='metric-label'>Avg Posts per User</div>
            </div>
            """, unsafe_allow_html=True)
            
        with col4:
            total_replies = data['Replies'].sum()
            st.markdown(f"""
            <div class='metric-card'>
                <div class='metric-value'>{total_replies}</div>
                <div class='metric-label'>Total Replies</div>
            </div>
            """, unsafe_allow_html=True)
        
        # User Activity Tab
        st.markdown("<h2 class='sub-header'>User Activity</h2>", unsafe_allow_html=True)
        
        # User posts over time 
        if not analysis['post_timeline'].empty:
            # Convert to datetime if needed
            if not pd.api.types.is_datetime64_dtype(analysis['post_timeline']['Datetime']):
                analysis['post_timeline']['Datetime'] = pd.to_datetime(analysis['post_timeline']['Datetime'])
            
            # Create the timeline
            fig = px.scatter(
                analysis['post_timeline'],
                x='Datetime',
                y='Username',
                color='Username',
                title='User Posting Timeline',
                height=500
            )
            
            fig.update_layout(
                xaxis_title='Date & Time',
                yaxis_title='Username',
                legend_title='Username',
                hovermode='closest'
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Add histogram of posts by hour of day
            if 'Datetime' in analysis['post_timeline'].columns:
                analysis['post_timeline']['Hour_of_Day'] = analysis['post_timeline']['Datetime'].dt.hour
                
                hour_counts = analysis['post_timeline'].groupby(['Hour_of_Day', 'Username']).size().reset_index(name='Count')
                
                fig_hour = px.bar(
                    hour_counts,
                    x='Hour_of_Day',
                    y='Count',
                    color='Username',
                    title='Post Distribution by Hour of Day',
                    labels={'Hour_of_Day': 'Hour (24-hour format)', 'Count': 'Number of Posts'},
                    height=400
                )
                
                st.plotly_chart(fig_hour, use_container_width=True)
        
        # User activity metrics
        if not analysis['user_activity'].empty:
            st.markdown("<h3>User Activity Metrics</h3>", unsafe_allow_html=True)
            st.dataframe(analysis['user_activity'].sort_values('Post_Count', ascending=False), use_container_width=True)
            
            # Visualization of post counts
            fig_posts = px.bar(
                analysis['user_activity'].sort_values('Post_Count', ascending=False),
                x='Username',
                y='Post_Count',
                title='Number of Posts per User',
                color='Username',
                height=400
            )
            
            st.plotly_chart(fig_posts, use_container_width=True)
        
        # Coordination Analysis
        st.markdown("<h2 class='sub-header'>Coordination Analysis</h2>", unsafe_allow_html=True)
        
        # Time-based coordination
        if not analysis['coordination_pairs'].empty:
            st.markdown("<h3>Users Posting in Close Time Proximity</h3>", unsafe_allow_html=True)
            
            # Count coordination pairs between users
            coord_counts = analysis['coordination_pairs'].groupby(['User1', 'User2']).size().reset_index(name='Count')
            coord_counts = coord_counts.sort_values('Count', ascending=False)
            
            st.dataframe(coord_counts.head(20), use_container_width=True)
            
            # Create network graph of coordination
            if len(coord_counts) > 0:
                # Only keep strong connections
                threshold = coord_counts['Count'].quantile(0.5)  # Adjust as needed
                strong_coords = coord_counts[coord_counts['Count'] > threshold]
                
                # Create graph
                G = nx.Graph()
                
                for _, row in strong_coords.iterrows():
                    G.add_edge(row['User1'], row['User2'], weight=row['Count'])
                
                # Create positions
                pos = nx.spring_layout(G, seed=42)
                
                # Create the figure
                fig = go.Figure()
                
                # Add edges
                edge_trace = []
                for edge in G.edges(data=True):
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]
                    weight = edge[2]['weight']
                    width = 1 + weight / strong_coords['Count'].max() * 5
                    
                    edge_trace.append(
                        go.Scatter(
                            x=[x0, x1, None],
                            y=[y0, y1, None],
                            line=dict(width=width, color='rgba(150,150,150,0.7)'),
                            hoverinfo='none',
                            mode='lines'
                        )
                    )
                
                for trace in edge_trace:
                    fig.add_trace(trace)
                
                # Add nodes
                node_adjacencies = []
                node_sizes = []
                for node in G.nodes():
                    adjacencies = len(list(G.neighbors(node)))
                    node_adjacencies.append(adjacencies)
                    post_count = analysis['user_activity'][analysis['user_activity']['Username'] == node]['Post_Count'].values[0] if node in analysis['user_activity']['Username'].values else 0
