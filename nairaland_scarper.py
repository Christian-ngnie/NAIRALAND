# -*- coding: utf-8 -*-
"""Nairaland_scarper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UXxE4eXsQHcLUbPX2tdUPqQXft2Uoc-6
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import re
import time
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
import concurrent.futures
from tqdm import tqdm

# Set page config
st.set_page_config(
    page_title="Nairaland User Coordination Analysis",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Title and description
st.title("Nairaland User Coordination Analysis Dashboard")
st.markdown("""
This dashboard scrapes and analyzes Nairaland user profiles to identify potential coordination patterns.
Extract data like usernames, registration dates, posts, publication times, and replies to detect inauthentic behavior.
""")

# Initialize session state if not already done
if 'scraped_data' not in st.session_state:
    st.session_state['scraped_data'] = None
if 'analyzed_data' not in st.session_state:
    st.session_state['analyzed_data'] = None
if 'network_data' not in st.session_state:
    st.session_state['network_data'] = None
if 'content_data' not in st.session_state:
    st.session_state['content_data'] = None

# Define helper functions for scraping
def scrape_profile(username):
    """Scrape profile data for a given username"""
    url = f"https://www.nairaland.com/{username}"
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            return {
                'username': username,
                'registration_date': 'Not Available',
                'last_seen': 'Not Available',
                'total_posts': 'Not Available',
                'total_topics': 'Not Available',
                'profile_info': 'Failed to scrape',
                'status': 'Error',
                'error_message': f'Status code: {response.status_code}'
            }

        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract profile data
        profile_data = {
            'username': username,
            'registration_date': None,
            'last_seen': None,
            'total_posts': None,
            'total_topics': None,
            'location': None,
            'personal_text': None,
            'gender': None,
            'profile_info': None,
            'status': 'Success',
            'url': url
        }

        # Find registration date
        reg_date_tr = soup.find('td', text=re.compile('Time registered', re.IGNORECASE))
        if reg_date_tr and reg_date_tr.find_next('td'):
            profile_data['registration_date'] = reg_date_tr.find_next('td').text.strip()

        # Find last seen
        last_seen_tr = soup.find('td', text=re.compile('Last seen', re.IGNORECASE))
        if last_seen_tr and last_seen_tr.find_next('td'):
            profile_data['last_seen'] = last_seen_tr.find_next('td').text.strip()

        # Find posts
        posts_link = soup.find('a', href=re.compile(f'/{username.lower()}/posts', re.IGNORECASE))
        if posts_link:
            posts_text = posts_link.text
            posts_count = re.search(r'\((\d+)\)', posts_text)
            if posts_count:
                profile_data['total_posts'] = posts_count.group(1)

        # Find topics
        topics_link = soup.find('a', href=re.compile(f'/{username.lower()}/topics', re.IGNORECASE))
        if topics_link:
            topics_text = topics_link.text
            topics_count = re.search(r'\((\d+)\)', topics_text)
            if topics_count:
                profile_data['total_topics'] = topics_count.group(1)

        # Find location
        location_tr = soup.find('td', text=re.compile('Location', re.IGNORECASE))
        if location_tr and location_tr.find_next('td'):
            profile_data['location'] = location_tr.find_next('td').text.strip()

        # Find personal text
        personal_text_tr = soup.find('td', text=re.compile('Personal text', re.IGNORECASE))
        if personal_text_tr and personal_text_tr.find_next('td'):
            profile_data['personal_text'] = personal_text_tr.find_next('td').text.strip()

        # Find gender
        gender_tr = soup.find('td', text=re.compile('Gender', re.IGNORECASE))
        if gender_tr and gender_tr.find_next('td'):
            profile_data['gender'] = gender_tr.find_next('td').text.strip()

        # Get the whole profile info for analysis
        profile_table = soup.find('table', {'summary': 'profile'})
        if profile_table:
            profile_data['profile_info'] = profile_table.get_text(strip=True)

        return profile_data

    except Exception as e:
        return {
            'username': username,
            'registration_date': 'Not Available',
            'last_seen': 'Not Available',
            'total_posts': 'Not Available',
            'total_topics': 'Not Available',
            'profile_info': 'Failed to scrape',
            'status': 'Error',
            'error_message': str(e)
        }

def scrape_posts(username, num_pages=2):
    """Scrape recent posts for a given username"""
    posts_data = []

    for page in range(0, num_pages):
        url = f"https://www.nairaland.com/{username}/posts/{page}"
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code != 200:
                continue

            soup = BeautifulSoup(response.text, 'html.parser')

            # Find all post containers
            post_tables = soup.find_all('table', class_='narrow')

            for table in post_tables:
                try:
                    # Extract post data
                    post_data = {
                        'username': username,
                        'post_title': None,
                        'post_content': None,
                        'post_date': None,
                        'post_time': None,
                        'post_topic': None,
                        'post_url': None
                    }

                    # Extract post title and topic
                    topic_link = table.find('a', class_='user')
                    if topic_link:
                        post_data['post_topic'] = topic_link.text.strip()
                        post_data['post_url'] = topic_link.get('href')

                    # Extract post content
                    post_body = table.find('div', class_='narrow')
                    if post_body:
                        post_data['post_content'] = post_body.get_text(strip=True)

                    # Extract date and time
                    post_time = table.find('span', class_='s')
                    if post_time:
                        datetime_text = post_time.text.strip()
                        # Parse date and time
                        date_match = re.search(r'(\d{1,2}:\d{2}[ap]m) On ([A-Za-z]{3} \d{1,2}, \d{4})', datetime_text)
                        if date_match:
                            post_data['post_time'] = date_match.group(1)
                            post_data['post_date'] = date_match.group(2)
                        else:
                            # Try alternative format
                            alt_date_match = re.search(r'(\d{1,2}:\d{2}[ap]m) On ([A-Za-z]{3} \d{1,2})', datetime_text)
                            if alt_date_match:
                                post_data['post_time'] = alt_date_match.group(1)
                                post_data['post_date'] = alt_date_match.group(2)

                    posts_data.append(post_data)
                except Exception as e:
                    continue

        except Exception as e:
            continue

    return posts_data

def standardize_date(date_str):
    """Convert various date formats to a standard format"""
    if not date_str or date_str == 'Not Available':
        return None

    # Try various date formats
    date_formats = [
        '%B %d, %Y',
        '%b %d, %Y',
        '%d %B %Y',
        '%d %b %Y'
    ]

    clean_date = date_str.strip()

    for fmt in date_formats:
        try:
            return datetime.strptime(clean_date, fmt).strftime('%Y-%m-%d')
        except:
            continue

    # If we cannot parse, return as is
    return date_str

def analyze_data(profiles_df, posts_df):
    """Analyze profile and post data for coordination patterns"""

    # Create analyzed dataframe
    analyzed_data = profiles_df.copy()

    # Convert registration_date to datetime for time-based analysis
    analyzed_data['registration_date_std'] = analyzed_data['registration_date'].apply(standardize_date)

    # Add metrics for analysis
    analyzed_data['total_posts'] = pd.to_numeric(analyzed_data['total_posts'], errors='coerce')
    analyzed_data['total_topics'] = pd.to_numeric(analyzed_data['total_topics'], errors='coerce')

    # Calculate posts per day (activity intensity)
    analyzed_data['activity_intensity'] = analyzed_data['total_posts'] / 30  # Rough estimate

    # Check for account creation clusters (accounts created around the same time)
    dates_count = analyzed_data['registration_date_std'].value_counts()
    analyzed_data['creation_cluster'] = analyzed_data['registration_date_std'].apply(
        lambda x: 'Cluster' if x and dates_count.get(x, 0) > 1 else 'No Cluster'
    )

    # Find username patterns (check for similar naming conventions)
    # Focus on WriterNig variants
    analyzed_data['writer_variant'] = analyzed_data['username'].apply(
        lambda x: 'Yes' if re.search(r'writ[et]+r?ni[g]+', x.lower()) else 'No'
    )

    # Add posts analysis if we have post data
    if not posts_df.empty:
        # Count posts per user
        posts_per_user = posts_df['username'].value_counts()
        analyzed_data['scraped_posts_count'] = analyzed_data['username'].apply(
            lambda x: posts_per_user.get(x, 0)
        )

        # Check for posting time patterns
        posts_df['hour'] = posts_df['post_time'].apply(
            lambda x: int(re.search(r'(\d+):', x).group(1)) if x and re.search(r'(\d+):', x) else None
        )

        # Get most common posting hour per user
        user_hours = {}
        for user in posts_df['username'].unique():
            user_posts = posts_df[posts_df['username'] == user]
            if not user_posts.empty and not user_posts['hour'].isna().all():
                hour_counts = user_posts['hour'].value_counts()
                if not hour_counts.empty:
                    user_hours[user] = hour_counts.index[0]
                else:
                    user_hours[user] = None
            else:
                user_hours[user] = None

        analyzed_data['common_posting_hour'] = analyzed_data['username'].apply(
            lambda x: user_hours.get(x)
        )

    # Calculate coordination likelihood based on multiple factors
    # 1. Username patterns
    # 2. Registration date clusters
    # 3. Activity patterns
    analyzed_data['coordination_likelihood'] = 0

    # Add points for writer variants
    analyzed_data.loc[analyzed_data['writer_variant'] == 'Yes', 'coordination_likelihood'] += 3

    # Add points for creation clusters
    analyzed_data.loc[analyzed_data['creation_cluster'] == 'Cluster', 'coordination_likelihood'] += 2

    # Add points for similar activity patterns (posts/topics ratio)
    if 'total_posts' in analyzed_data.columns and 'total_topics' in analyzed_data.columns:
        analyzed_data['post_topic_ratio'] = analyzed_data['total_posts'] / analyzed_data['total_topics'].replace(0, 1)
        # Group by similar ratios
        ratio_mean = analyzed_data['post_topic_ratio'].mean()
        ratio_std = analyzed_data['post_topic_ratio'].std()
        analyzed_data['similar_ratio'] = abs(analyzed_data['post_topic_ratio'] - ratio_mean) < ratio_std
        analyzed_data.loc[analyzed_data['similar_ratio'] == True, 'coordination_likelihood'] += 1

    # Convert to categorical
    analyzed_data['coordination_level'] = pd.cut(
        analyzed_data['coordination_likelihood'],
        bins=[-1, 0, 2, 4, float('inf')],
        labels=['None', 'Low', 'Medium', 'High']
    )

    return analyzed_data

def build_network_data(analyzed_data, posts_df):
    """Build network data for visualization"""

    # Create network nodes (users)
    nodes = []
    for _, row in analyzed_data.iterrows():
        nodes.append({
            'id': row['username'],
            'username': row['username'],
            'posts': row['total_posts'] if pd.notna(row['total_posts']) else 0,
            'topics': row['total_topics'] if pd.notna(row['total_topics']) else 0,
            'coordination': row['coordination_likelihood'] if pd.notna(row['coordination_likelihood']) else 0,
            'group': 1 if row['writer_variant'] == 'Yes' else 2
        })

    # Create edges (connections between users)
    edges = []

    # Connect users in the same creation cluster
    clusters = analyzed_data[analyzed_data['creation_cluster'] == 'Cluster']['registration_date_std'].dropna().unique()

    for cluster in clusters:
        cluster_users = analyzed_data[analyzed_data['registration_date_std'] == cluster]['username'].tolist()
        for i in range(len(cluster_users)):
            for j in range(i+1, len(cluster_users)):
                edges.append({
                    'source': cluster_users[i],
                    'target': cluster_users[j],
                    'type': 'registration_cluster',
                    'weight': 2
                })

    # Connect writer variants
    writer_variants = analyzed_data[analyzed_data['writer_variant'] == 'Yes']['username'].tolist()
    for i in range(len(writer_variants)):
        for j in range(i+1, len(writer_variants)):
            edges.append({
                'source': writer_variants[i],
                'target': writer_variants[j],
                'type': 'naming_pattern',
                'weight': 3
            })

    # Connect users with similar posting patterns (if we have post data)
    if not posts_df.empty and 'common_posting_hour' in analyzed_data.columns:
        # Group by common posting hour
        for hour in analyzed_data['common_posting_hour'].dropna().unique():
            hour_users = analyzed_data[analyzed_data['common_posting_hour'] == hour]['username'].tolist()
            for i in range(len(hour_users)):
                for j in range(i+1, len(hour_users)):
                    edges.append({
                        'source': hour_users[i],
                        'target': hour_users[j],
                        'type': 'posting_pattern',
                        'weight': 1
                    })

    return {'nodes': nodes, 'edges': edges}

def analyze_content(posts_df):
    """Analyze post content for common themes and coordination"""
    if posts_df.empty:
        return None

    # Initialize NLTK if needed
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
        nltk.download('punkt')

    stop_words = set(stopwords.words('english'))

    # Process text content
    all_content = ' '.join(posts_df['post_content'].dropna().astype(str).tolist())
    tokens = word_tokenize(all_content.lower())
    filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words and len(w) > 3]

    # Get word frequencies
    word_freq = Counter(filtered_tokens)
    common_words = word_freq.most_common(50)

    # Get themes per user
    user_themes = {}
    for user in posts_df['username'].unique():
        user_posts = posts_df[posts_df['username'] == user]
        if not user_posts.empty:
            user_content = ' '.join(user_posts['post_content'].dropna().astype(str).tolist())
            user_tokens = word_tokenize(user_content.lower())
            user_filtered = [w for w in user_tokens if w.isalpha() and w not in stop_words and len(w) > 3]
            user_freq = Counter(user_filtered)
            user_themes[user] = user_freq.most_common(10)

    # Calculate theme similarity between users
    theme_similarity = []
    users = list(user_themes.keys())
    for i in range(len(users)):
        for j in range(i+1, len(users)):
            user1 = users[i]
            user2 = users[j]

            # Get word sets
            words1 = {word for word, _ in user_themes[user1]}
            words2 = {word for word, _ in user_themes[user2]}

            # Calculate Jaccard similarity
            if words1 and words2:
                similarity = len(words1.intersection(words2)) / len(words1.union(words2))

                if similarity > 0.2:  # Only include significant similarities
                    theme_similarity.append({
                        'user1': user1,
                        'user2': user2,
                        'similarity': similarity,
                        'common_themes': list(words1.intersection(words2))
                    })

    return {
        'word_frequencies': common_words,
        'user_themes': user_themes,
        'theme_similarity': theme_similarity
    }

# Streamlit UI
with st.sidebar:
    st.header("Control Panel")

    # Input for usernames
    default_usernames = """
    elusive001
    botragelad
    holiness2100
    uprightness100
    truthU87
    biodun556
    coronaVirusPro
    NigerianXXX
    Kingsnairaland
    Betscoreodds
    Nancy2020
    Nancy1986
    Writernig
    WritterNg
    WriiterNg
    WrriterNg
    WriteerNig
    WriterrNig
    WritterNig
    WriiterNig
    WrriterNig
    WriterNigg
    WriterNiiig
    WriterNiig
    Ken6488
    Dalil8
    Slavaukraini
    Redscorpion
    Nigeriazoo
    """

    usernames_input = st.text_area(
        "Enter usernames (one per line):",
        value=default_usernames,
        height=300
    )

    # Parse usernames
    usernames = [u.strip() for u in usernames_input.split('\n') if u.strip()]

    # Scraping options
    st.subheader("Scraping Options")
    scrape_posts_option = st.checkbox("Scrape recent posts (slower)", value=True)
    posts_pages = st.slider("Number of post pages to scrape", 1, 5, 2)

    # Action buttons
    start_scraping = st.button("Start Scraping")

    # Display info
    st.info(f"Number of profiles to scrape: {len(usernames)}")

# Main area
tab1, tab2, tab3, tab4, tab5 = st.tabs(["Overview", "User Profiles", "Post Analysis", "Network Visualization", "Content Analysis"])

# Handle scraping process
if start_scraping:
    with st.spinner('Scraping profiles...'):
        # Initialize progress
        progress_text = "Scraping profiles..."
        progress_bar = st.progress(0)

        # Scrape profiles
        profiles_data = []

        for i, username in enumerate(usernames):
            profile = scrape_profile(username)
            profiles_data.append(profile)

            # Update progress
            progress_bar.progress((i + 1) / len(usernames))
            time.sleep(0.1)  # To avoid too many requests at once

        profiles_df = pd.DataFrame(profiles_data)

        # Scrape posts if enabled
        posts_data = []
        if scrape_posts_option:
            progress_text = "Scraping posts..."
            progress_bar.progress(0)

            for i, username in enumerate(usernames):
                user_posts = scrape_posts(username, num_pages=posts_pages)
                posts_data.extend(user_posts)

                # Update progress
                progress_bar.progress((i + 1) / len(usernames))
                time.sleep(0.2)  # To avoid too many requests at once

        posts_df = pd.DataFrame(posts_data)

        # Store data in session state
        st.session_state['scraped_data'] = {
            'profiles': profiles_df,
            'posts': posts_df
        }

        # Analyze data
        analyzed_data = analyze_data(profiles_df, posts_df)
        st.session_state['analyzed_data'] = analyzed_data

        # Build network data
        network_data = build_network_data(analyzed_data, posts_df)
        st.session_state['network_data'] = network_data

        # Analyze content
        content_data = analyze_content(posts_df)
        st.session_state['content_data'] = content_data

        # Hide progress bar when complete
        progress_bar.empty()

        st.success('Scraping and analysis complete!')

# Display data if it exists
if st.session_state['scraped_data'] is not None:
    scraped_data = st.session_state['scraped_data']
    profiles_df = scraped_data['profiles']
    posts_df = scraped_data['posts']

    analyzed_data = st.session_state['analyzed_data']
    network_data = st.session_state['network_data']
    content_data = st.session_state['content_data']

    # Tab 1: Overview
    with tab1:
        st.header("Analysis Overview")

        # Summary metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Users", len(profiles_df))
        with col2:
            high_coord = len(analyzed_data[analyzed_data['coordination_level'] == 'High'])
            st.metric("High Coordination", high_coord)
        with col3:
            writer_variants = len(analyzed_data[analyzed_data['writer_variant'] == 'Yes'])
            st.metric("WriterNig Variants", writer_variants)
        with col4:
            total_posts = analyzed_data['total_posts'].sum()
            st.metric("Total Posts", f"{int(total_posts):,}")

        # Coordination distribution
        st.subheader("Coordination Level Distribution")
        coord_counts = analyzed_data['coordination_level'].value_counts().reset_index()
        coord_counts.columns = ['Coordination Level', 'Count']

        fig = px.bar(
            coord_counts,
            x='Coordination Level',
            y='Count',
            color='Coordination Level',
            color_discrete_map={
                'None': 'green',
                'Low': 'yellow',
                'Medium': 'orange',
                'High': 'red'
            }
        )
        st.plotly_chart(fig, use_container_width=True)

        # Key findings
        st.subheader("Key Findings")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### Registration Date Clusters")
            # Check for registration date clusters
            reg_date_counts = analyzed_data['registration_date_std'].value_counts().reset_index()
            reg_date_counts.columns = ['Registration Date', 'Count']
            reg_date_counts = reg_date_counts[reg_date_counts['Count'] > 1]

            if not reg_date_counts.empty:
                fig = px.bar(
                    reg_date_counts,
                    x='Registration Date',
                    y='Count',
                    title="Users registered on the same date"
                )
                st.plotly_chart(fig, use_container_width=True)

        with col2:
            st.markdown("#### Coordination Network")

            # Create simplified network viz
            G = nx.Graph()

            # Add nodes
            for node in network_data['nodes']:
                G.add_node(node['id'], group=node['group'])

            # Add edges (only strong connections)
            for edge in network_data['edges']:
                if edge['weight'] >= 2:  # Only strong connections
                    G.add_edge(edge['source'], edge['target'])

            # Draw the graph
            pos = nx.spring_layout(G, seed=42)

            # Get node colors
            node_groups = [G.nodes[node]['group'] for node in G.nodes()]
            node_colors = ['red' if group == 1 else 'blue' for group in node_groups]

            fig, ax = plt.subplots(figsize=(8, 6))
            nx.draw_networkx(
                G, pos, ax=ax,
                with_labels=True,
                node_color=node_colors,
                node_size=200,
                font_size=10,
                edge_color='gray',
                alpha=0.8
            )
            plt.axis('off')
            st.pyplot(fig)

    # Tab 2: User Profiles
    with tab2:
        st.header("User Profile Data")

        # Filter options
        st.subheader("Filter Options")
        coord_filter = st.multiselect(
            "Filter by Coordination Level:",
            options=['None', 'Low', 'Medium', 'High'],
            default=['Medium', 'High']
        )

        # Apply filters
        filtered_df = analyzed_data
        if coord_filter:
            filtered_df = filtered_df[filtered_df['coordination_level'].isin(coord_filter)]

        # Display in format
        st.dataframe(
            filtered_df[[
                'username', 'registration_date', 'last_seen',
                'total_posts', 'total_topics', 'writer_variant',
                'coordination_level'
            ]],
            height=400,
            use_container_width=True
        )

        # User details
        st.subheader("User Details")
        selected_user = st.selectbox("Select User:", analyzed_data['username'].tolist())

        if selected_user:
            user_data = analyzed_data[analyzed_data['username'] == selected_user].iloc[0]

            col1, col2 = st.columns(2)

            with col1:
                st.markdown(f"#### {selected_user}")
                st.markdown(f"**Registration Date:** {user_data['registration_date']}")
                st.markdown(f"**Last Seen:** {user_data['last_seen']}")
                st.markdown(f"**Total Posts:** {user_data['total_posts']}")
                st.markdown(f"**Total Topics:** {user_data['total_topics']}")

                if not pd.isna(user_data.get('location')):
                    st.markdown(f"**Location:** {user_data['location']}")

                if not pd.isna(user_data.get('personal_text')):
                    st.markdown(f"**Personal Text:** {user_data['personal_text']}")

            with col2:
                st.markdown("#### Coordination Analysis")
                st.markdown(f"**Coordination Level:** {user_data['coordination_level']}")
                st.markdown(f"**WriterNig Variant:** {user_data['writer_variant']}")
                st.markdown(f"**Creation Cluster:** {user_data['creation_cluster']}")

                if 'common_posting_hour' in user_data and not pd.isna(user_data['common_posting_hour']):
                    st.markdown(f"**Common Posting Hour:** {user_data['common_posting_hour']}:00")

            # Display similar users
            st.subheader("Similar Users")

            if network_data:
                similar_users = []

                for edge in network_data['edges']:
                    if edge['source'] == selected_user:
                        similar_users.append({
                            'username': edge['target'],
                            'connection_type': edge['type'],
                            'strength': edge['weight']
                        })
                    elif edge['target'] == selected_user:
                        similar_users.append({
                            'username': edge['source'],
                            'connection_type': edge['type'],
                            'strength': edge['weight']
                        })

                if similar_users:
                    st.dataframe(
                        pd.DataFrame(similar_users),
                        use_container_width=True
                    )
                else:
                    st.info("No similar users found.")

    # Tab 3: Post Analysis
    with tab3:
        st.header("Post Analysis")

        if not posts_df.empty:
            # Filter options
            st.subheader("Filter Options")
            post_user_filter = st.multiselect(
                "Filter by User:",
                options=posts_df['username'].unique().tolist(),
                default=[]
            )

            # Apply filters
            filtered_posts = posts_df
            if post_user_filter:
                filtered_posts = filtered_posts[filtered_posts['username'].isin(post_user_filter)]