# -*- coding: utf-8 -*-
"""Nairaland_scarper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UXxE4eXsQHcLUbPX2tdUPqQXft2Uoc-6
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from datetime import datetime
import re
from collections import Counter
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download NLTK resources
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

# Set page configuration
st.set_page_config(
    page_title="Nairaland User Coordination Analysis",
    page_icon="ðŸ“Š",
    layout="wide"
)

# Dashboard title
st.title("Nairaland User Coordination Analysis Dashboard")
st.write("This dashboard scrapes data from Nairaland user profiles and analyzes coordination patterns.")

# Sidebar for inputs
with st.sidebar:
    st.header("Input URLs")
    urls_input = st.text_area(
        "Enter Nairaland profile URLs (one per line)",
        """https://www.nairaland.com/elusive001
https://www.nairaland.com/botragelad
https://www.nairaland.com/holiness2100
https://www.nairaland.com/uprightness100
https://www.nairaland.com/truthU87
https://www.nairaland.com/biodun556
https://www.nairaland.com/coronaVirusPro
https://www.nairaland.com/NigerianXXX
https://www.nairaland.com/Kingsnairaland
https://www.nairaland.com/Betscoreodds
https://www.nairaland.com/Nancy2020
https://www.nairaland.com/Nancy1986
https://www.nairaland.com/Writernig
https://www.nairaland.com/WritterNg
https://www.nairaland.com/WriiterNg
https://www.nairaland.com/WrriterNg
https://www.nairaland.com/WriteerNig
https://www.nairaland.com/WriterrNig
https://www.nairaland.com/WritterNig
https://www.nairaland.com/WriiterNig
https://www.nairaland.com/WrriterNig
https://www.nairaland.com/WriterNigg
https://www.nairaland.com/WriterNiiig
https://www.nairaland.com/WriterNiig
https://www.nairaland.com/Ken6488
https://www.nairaland.com/Dalil8
https://www.nairaland.com/Slavaukraini
https://www.nairaland.com/Redscorpion
https://www.nairaland.com/Nigeriazoo"""
    )
    
    # Extract number of posts to scrape per user
    max_posts = st.slider("Maximum posts to scrape per user", 10, 200, 50)
    
    # Start scraping button
    start_scraping = st.button("Start Scraping and Analysis")

# Function to scrape profile info
def scrape_profile_info(url):
    try:
        st.write(f"Scraping profile: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        username = url.split('/')[-1]
        
        # Extract registration date
        reg_date_text = None
        time_elements = soup.find_all('td', text=re.compile('Time registered'))
        if time_elements:
            reg_date_text = time_elements[0].find_next_sibling('td').text.strip()
        
        # Extract location
        location = None
        location_elements = soup.find_all('td', text=re.compile('Location'))
        if location_elements:
            location = location_elements[0].find_next_sibling('td').text.strip()
        
        # Extract time spent online
        time_online = None
        online_elements = soup.find_all('td', text=re.compile('Time spent online'))
        if online_elements:
            time_online = online_elements[0].find_next_sibling('td').text.strip()
        
        profile_data = {
            'username': username,
            'registration_date': reg_date_text,
            'location': location,
            'time_online': time_online,
            'url': url
        }
        
        # Get link to posts
        posts_url = f"{url}/posts"
        
        return profile_data, posts_url
    
    except Exception as e:
        st.error(f"Error scraping profile {url}: {str(e)}")
        return None, None

# Function to scrape posts
def scrape_posts(posts_url, max_posts=50):
    posts = []
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(posts_url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Find all post divs
        post_elements = soup.find_all(['div', 'td'], class_=None, id=None)
        
        count = 0
        for element in post_elements:
            if count >= max_posts:
                break
                
            # Look for post structure patterns
            topic_link = element.find('a', href=re.compile(r'/\d+/'))
            date_time_span = element.find(text=re.compile(r'(On [A-Za-z]+ \d+|[0-9]+:[0-9]+[ap]m)'))
            
            if topic_link and date_time_span:
                # Found a post
                topic = topic_link.text.strip()
                
                # Extract content (all text that's not in another special element)
                content_parts = []
                for item in element.contents:
                    if isinstance(item, str) and item.strip():
                        content_parts.append(item.strip())
                
                content = " ".join(content_parts)
                if not content and element.get_text():
                    content = element.get_text().strip()
                
                # Extract date and time
                date_match = re.search(r'On ([A-Za-z]+ \d+)', str(element))
                time_match = re.search(r'([0-9]+:[0-9]+[ap]m)', str(element))
                
                post_date = date_match.group(1) if date_match else "Unknown"
                post_time = time_match.group(1) if time_match else "Unknown"
                
                # Extract likes and shares if available
                likes = 0
                shares = 0
                likes_match = re.search(r'(\d+) Like', str(element))
                shares_match = re.search(r'(\d+) Share', str(element))
                
                if likes_match:
                    likes = int(likes_match.group(1))
                if shares_match:
                    shares = int(shares_match.group(1))
                
                posts.append({
                    'topic': topic,
                    'content': content,
                    'date': post_date,
                    'time': post_time,
                    'likes': likes,
                    'shares': shares
                })
                count += 1
        
        return posts
    
    except Exception as e:
        st.error(f"Error scraping posts from {posts_url}: {str(e)}")
        return []

# Main analysis functions
def analyze_temporal_patterns(user_posts_df):
    st.subheader("Temporal Posting Analysis")
    
    # Convert date and time to datetime
    user_posts_df['datetime'] = pd.to_datetime(
        user_posts_df['date'] + ' ' + user_posts_df['time'], 
        format='%b %d %I:%M%p', 
        errors='coerce'
    )
    
    # Extract hour of day
    user_posts_df['hour'] = user_posts_df['datetime'].dt.hour
    
    # Posting frequency by hour
    hour_counts = user_posts_df['hour'].value_counts().sort_index()
    
    # Plot posting time distribution
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(hour_counts.index, hour_counts.values)
    ax.set_xlabel('Hour of Day (24h format)')
    ax.set_ylabel('Number of Posts')
    ax.set_title('Posting Time Distribution')
    ax.set_xticks(range(0, 24))
    st.pyplot(fig)
    
    # Posting patterns by user
    user_hour_matrix = pd.crosstab(user_posts_df['username'], user_posts_df['hour'])
    
    # Heatmap of user posting times
    fig, ax = plt.subplots(figsize=(12, len(user_hour_matrix) * 0.4 + 2))
    sns.heatmap(user_hour_matrix, cmap="YlGnBu", ax=ax)
    ax.set_title('User Posting Times Heatmap')
    ax.set_xlabel('Hour of Day')
    ax.set_ylabel('Username')
    st.pyplot(fig)
    
    st.write("### Coordination Insights:")
    st.write("""
    - Users posting at similar times may indicate coordination
    - Clusters of activity at unusual hours (like late night/early morning) can suggest automated behavior
    - Synchronized posting patterns across multiple accounts may indicate a single operator
    """)
    
    return user_hour_matrix

def analyze_content_similarity(user_posts_df):
    st.subheader("Content Similarity Analysis")
    
    # Aggregate all posts by user
    user_content = user_posts_df.groupby('username')['content'].apply(' '.join).reset_index()
    
    # Create TF-IDF vectors
    stop_words = set(stopwords.words('english'))
    vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)
    
    # Check if we have enough content
    if len(user_content) > 1:
        tfidf_matrix = vectorizer.fit_transform(user_content['content'])
        
        # Calculate cosine similarity
        cosine_sim = cosine_similarity(tfidf_matrix)
        
        # Create similarity DataFrame
        similarity_df = pd.DataFrame(
            cosine_sim, 
            index=user_content['username'], 
            columns=user_content['username']
        )
        
        # Plot heatmap
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(similarity_df, annot=True, cmap='YlGnBu', vmin=0, vmax=1, ax=ax)
        ax.set_title('Content Similarity Between Users')
        st.pyplot(fig)
        
        # Identify highly similar users
        high_similarity_pairs = []
        for i in range(len(similarity_df.index)):
            for j in range(i+1, len(similarity_df.columns)):
                if similarity_df.iloc[i, j] > 0.7:  # Threshold for high similarity
                    high_similarity_pairs.append((
                        similarity_df.index[i],
                        similarity_df.columns[j],
                        similarity_df.iloc[i, j]
                    ))
        
        if high_similarity_pairs:
            st.write("### High Similarity User Pairs:")
            for user1, user2, sim in sorted(high_similarity_pairs, key=lambda x: x[2], reverse=True):
                st.write(f"- {user1} and {user2}: {sim:.2f} similarity score")
                
            # Create network visualization for similar users
            G = nx.Graph()
            for user in user_content['username']:
                G.add_node(user)
            
            for user1, user2, sim in high_similarity_pairs:
                G.add_edge(user1, user2, weight=sim)
            
            # Plot network
            fig, ax = plt.subplots(figsize=(12, 10))
            pos = nx.spring_layout(G, seed=42)
            nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue', ax=ax)
            nx.draw_networkx_labels(G, pos, font_size=12, ax=ax)
            
            # Draw edges with width based on similarity
            for u, v, d in G.edges(data=True):
                nx.draw_networkx_edges(
                    G, pos, edgelist=[(u, v)], 
                    width=d['weight'] * 5, 
                    alpha=0.7, edge_color='gray', 
                    ax=ax
                )
            
            plt.title("User Similarity Network")
            plt.axis('off')
            st.pyplot(fig)
        else:
            st.write("No highly similar users detected based on content.")
    else:
        st.write("Not enough users with content to perform similarity analysis.")
    
    # Common phrases analysis
    st.subheader("Common Phrases Analysis")
    
    # Function to extract ngrams
    def extract_ngrams(text, n=2):
        tokens = word_tokenize(text.lower())
        tokens = [token for token in tokens if token.isalnum() and token not in stop_words]
        ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
        return ngrams
    
    # Extract common phrases across users
    all_bigrams = []
    for content in user_posts_df['content']:
        if isinstance(content, str):
            all_bigrams.extend(extract_ngrams(content, 2))
    
    # Get most common phrases
    common_phrases = Counter(all_bigrams).most_common(20)
    
    # Display common phrases
    if common_phrases:
        phrase_df = pd.DataFrame(common_phrases, columns=['Phrase', 'Count'])
        
        # Plot common phrases
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.barplot(x='Count', y='Phrase', data=phrase_df, ax=ax)
        ax.set_title('Most Common Phrases Across Users')
        st.pyplot(fig)
        
        st.write("""
        ### Content Analysis Insights:
        - High similarity between accounts suggests potential coordination or the same operator
        - Common unusual phrases across accounts may indicate shared talking points
        - Similar formatting, hashtags, or unique expressions can reveal coordination
        """)
    else:
        st.write("No common phrases found across users.")
    
    return similarity_df if 'similarity_df' in locals() else None

def analyze_account_creation_patterns(profiles_df):
    st.subheader("Account Creation Timeline Analysis")
    
    # Convert registration dates to datetime
    profiles_df['reg_date_dt'] = pd.to_datetime(
        profiles_df['registration_date'], 
        format='%B %d, %Y', 
        errors='coerce'
    )
    
    # Sort by registration date
    sorted_profiles = profiles_df.sort_values('reg_date_dt')
    
    # Plot account creation timeline
    if not sorted_profiles['reg_date_dt'].isna().all():
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.scatter(
            sorted_profiles['reg_date_dt'], 
            range(len(sorted_profiles)), 
            s=100, alpha=0.7
        )
        
        # Add labels for each point
        for i, (idx, row) in enumerate(sorted_profiles.iterrows()):
            if not pd.isna(row['reg_date_dt']):
                ax.text(
                    row['reg_date_dt'], i, 
                    f"  {row['username']}", 
                    va='center', fontsize=9
                )
        
        ax.set_yticks([])
        ax.set_xlabel('Registration Date')
        ax.set_title('Account Creation Timeline')
        ax.grid(True, linestyle='--', alpha=0.7)
        
        # Identify clusters of account creations
        profiles_df['reg_date_dt_ordinal'] = profiles_df['reg_date_dt'].map(lambda x: x.toordinal() if not pd.isna(x) else None)
        date_diffs = []
        
        for i in range(1, len(sorted_profiles)):
            current_date = sorted_profiles.iloc[i]['reg_date_dt_ordinal']
            prev_date = sorted_profiles.iloc[i-1]['reg_date_dt_ordinal']
            
            if current_date is not None and prev_date is not None:
                date_diffs.append((
                    sorted_profiles.iloc[i-1]['username'],
                    sorted_profiles.iloc[i]['username'],
                    current_date - prev_date
                ))
        
        # Identify accounts created close together (within 3 days)
        close_creation = [(u1, u2, diff) for u1, u2, diff in date_diffs if diff <= 3]
        
        if close_creation:
            st.write("### Accounts Created Within 3 Days of Each Other:")
            for u1, u2, diff in close_creation:
                st.write(f"- {u1} and {u2}: {diff} days apart")
        
        st.pyplot(fig)
        
        st.write("""
        ### Account Creation Insights:
        - Multiple accounts created in a short timeframe may indicate coordinated creation
        - Batches of accounts created around specific events can signal preparation for influence operations
        - Sequential naming patterns with close creation dates strongly suggest the same operator
        """)
    else:
        st.write("Could not parse registration dates for timeline analysis.")

def analyze_naming_patterns(profiles_df):
    st.subheader("Username Pattern Analysis")
    
    # Group similar usernames
    username_patterns = {}
    
    # Look for number patterns
    num_pattern = re.compile(r'.*?(\d+)$')
    num_users = []
    
    for username in profiles_df['username']:
        match = num_pattern.match(username)
        if match:
            num_users.append((username, match.group(1)))
    
    if num_users:
        st.write("### Usernames with Numerical Patterns:")
        for username, num in num_users:
            st.write(f"- {username}: ends with {num}")
    
    # Look for common prefixes
    prefixes = {}
    for username in profiles_df['username']:
        for length in range(3, 7):  # Check prefixes of length 3-6
            if len(username) >= length:
                prefix = username[:length].lower()
                if prefix not in prefixes:
                    prefixes[prefix] = []
                prefixes[prefix].append(username)
    
    # Filter for prefixes with multiple usernames
    common_prefixes = {prefix: users for prefix, users in prefixes.items() if len(users) > 1}
    
    if common_prefixes:
        st.write("### Usernames with Common Prefixes:")
        for prefix, users in common_prefixes.items():
            st.write(f"- Prefix '{prefix}': {', '.join(users)}")
    
    # Check for Levenshtein distance (string similarity)
    from Levenshtein import distance
    
    similar_names = []
    usernames = profiles_df['username'].tolist()
    
    for i in range(len(usernames)):
        for j in range(i+1, len(usernames)):
            dist = distance(usernames[i].lower(), usernames[j].lower())
            if dist <= 2 and dist > 0:  # Names very similar but not identical
                similar_names.append((usernames[i], usernames[j], dist))
    
    if similar_names:
        st.write("### Similar Usernames (Levenshtein Distance â‰¤ 2):")
        for name1, name2, dist in similar_names:
            st.write(f"- {name1} and {name2}: distance = {dist}")
    
    st.write("""
    ### Username Pattern Insights:
    - Sequential numbers in usernames may indicate batch account creation
    - Very similar usernames with minor variations often belong to the same operator
    - Consistent naming patterns across accounts suggest coordinated creation
    """)

def analyze_activity_metrics(profiles_df, user_posts_df):
    st.subheader("User Activity Metrics")
    
    # Calculate posts per day since registration
    profiles_df['days_since_reg'] = None
    
    for idx, row in profiles_df.iterrows():
        if pd.notna(row['reg_date_dt']):
            days = (datetime.now() - row['reg_date_dt']).days
            profiles_df.at[idx, 'days_since_reg'] = max(1, days)  # Avoid division by zero
    
    # Count posts per user
    post_counts = user_posts_df['username'].value_counts().reset_index()
    post_counts.columns = ['username', 'post_count']
    
    # Merge with profiles
    activity_df = profiles_df.merge(post_counts, on='username', how='left')
    activity_df['post_count'] = activity_df['post_count'].fillna(0)
    
    # Calculate posts per day
    activity_df['posts_per_day'] = activity_df.apply(
        lambda row: row['post_count'] / row['days_since_reg'] if pd.notna(row['days_since_reg']) else None, 
        axis=1
    )
    
    # Display activity metrics
    if not activity_df['posts_per_day'].isna().all():
        # Plot posts per day
        fig, ax = plt.subplots(figsize=(12, 8))
        activity_plot = activity_df.sort_values('posts_per_day', ascending=False)
        
        sns.barplot(
            x='posts_per_day', 
            y='username', 
            data=activity_plot.head(15),  # Top 15 most active users
            ax=ax
        )
        
        ax.set_title('Posts Per Day Since Registration')
        ax.set_xlabel('Average Posts Per Day')
        ax.set_ylabel('Username')
        st.pyplot(fig)
        
        st.write("""
        ### Activity Metrics Insights:
        - Unusually high posting rates may indicate automated activity
        - Similar posting frequencies across multiple accounts suggest coordination
        - Accounts with synchronized activity spikes often have a common operator
        """)
    else:
        st.write("Insufficient data to calculate posting frequency.")

# Main execution
if start_scraping:
    urls = urls_input.strip().split('\n')
    
    # Create progress bar
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Lists to store results
    all_profiles = []
    all_posts = []
    
    # Scrape each profile
    for i, url in enumerate(urls):
        status_text.text(f"Scraping {url}...")
        profile_data, posts_url = scrape_profile_info(url)
        
        if profile_data and posts_url:
            all_profiles.append(profile_data)
            
            # Scrape posts
            status_text.text(f"Scraping posts for {profile_data['username']}...")
            user_posts = scrape_posts(posts_url, max_posts)
            
            # Add username to posts
            for post in user_posts:
                post['username'] = profile_data['username']
            
            all_posts.extend(user_posts)
        
        # Update progress
        progress_value = (i + 1) / len(urls)
        progress_bar.progress(progress_value)
        time.sleep(0.1)  # Small delay to avoid overwhelming the server
    
    # Convert to DataFrames
    profiles_df = pd.DataFrame(all_profiles)
    user_posts_df = pd.DataFrame(all_posts)
    
    # Display scraped data
    st.header("Scraped Data Overview")
    
    # Profile information
    st.subheader("User Profiles")
    st.dataframe(profiles_df)
    
    # Post information
    st.subheader("User Posts")
    st.dataframe(user_posts_df.head(20))  # Show just the first 20 for brevity
    
    # Run analysis functions
    st.header("Coordination Pattern Analysis")
    
    # Display analysis tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "Temporal Patterns", 
        "Content Similarity", 
        "Account Creation", 
        "Username Patterns",
        "Activity Metrics"
    ])
    
    with tab1:
        analyze_temporal_patterns(user_posts_df)
    
    with tab2:
        analyze_content_similarity(user_posts_df)
    
    with tab3:
        analyze_account_creation_patterns(profiles_df)
    
    with tab4:
        analyze_naming_patterns(profiles_df)
    
    with tab5:
        analyze_activity_metrics(profiles_df, user_posts_df)
    
    # Final summary
    st.header("Summary of Coordination Findings")
    st.write("""
    Based on the analysis, potential coordination patterns have been identified:
    
    1. **Temporal Coordination**: 
       - Users posting at similar times or showing synchronized activity patterns
       - Accounts active during unusual hours (potential automation)
    
    2. **Content Coordination**:
       - Accounts sharing similar content, phrasing, or talking points
       - Common narrative themes across multiple accounts
    
    3. **Account Patterns**:
       - Clusters of accounts created in close succession
       - Similar naming patterns suggesting common operator
       - Comparable activity levels across related accounts
    
    The dashboard provides multiple visualizations to help identify these patterns and understand the relationships between the analyzed accounts.
    """)
