# -*- coding: utf-8 -*-
"""Nairaland_scarper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UXxE4eXsQHcLUbPX2tdUPqQXft2Uoc-6
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from bs4 import BeautifulSoup
import re
import time
import random
from datetime import datetime, timedelta
import cloudscraper
import networkx as nx
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
import logging
import os
import json
import warnings
import hashlib
from urllib.parse import urlparse, urljoin

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Ignore warnings
warnings.filterwarnings("ignore")

# Download NLTK resources
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    pass

# Set page configuration
st.set_page_config(
    page_title="Nairaland Account Analysis Dashboard",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Define color scheme
COLORS = {
    'primary': '#1E88E5',
    'secondary': '#FFC107',
    'background': '#F5F5F5',
    'text': '#212121',
    'accent': '#FF5722',
}

# CSS for styling
st.markdown("""
<style>
    .main {
        background-color: #F5F5F5;
    }
    .st-bx {
        background-color: #FFFFFF;
    }
    .css-1aumxhk {
        background-color: #1E88E5;
        color: white !important;
    }
    .css-pkbazv {
        color: #212121;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 10px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: #FFFFFF;
        border-radius: 5px 5px 0px 0px;
        border-right: 1px solid #EEEEEE;
        border-left: 1px solid #EEEEEE;
        border-top: 1px solid #EEEEEE;
    }
    .stTabs [aria-selected="true"] {
        background-color: #1E88E5;
        color: white !important;
    }
    div.stButton > button:first-child {
        background-color: #1E88E5;
        color: white;
        border: none;
    }
    div.stButton > button:hover {
        background-color: #0D47A1;
        color: white;
        border: none;
    }
    .reportview-container .main .block-container {
        padding-top: 1rem;
        padding-right: 1rem;
        padding-left: 1rem;
        padding-bottom: 1rem;
    }
    .css-18e3th9 {
        padding: 1rem 1rem 1rem 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Create a sidebar for navigation
st.sidebar.image("https://www.nairaland.com/static/logo1.png", width=200)
st.sidebar.title("Navigation")

# Create a sidebar for settings
st.sidebar.header("Settings")

# Add a file uploader for a proxy list
proxy_file = st.sidebar.file_uploader("Upload proxy list (optional)", type=["txt"])
proxies = []
if proxy_file:
    proxy_content = proxy_file.read().decode("utf-8")
    proxies = [line.strip() for line in proxy_content.split("\n") if line.strip()]
    st.sidebar.success(f"Loaded {len(proxies)} proxies")

# User-Agent List
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 11.5; rv:90.0) Gecko/20100101 Firefox/90.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15',
]

# Create a directory for cache if it doesn't exist
if not os.path.exists('cache'):
    os.makedirs('cache')

class NairalandScraper:
    def __init__(self, user_agents, proxies=None, delay_range=(1, 3)):
        self.user_agents = user_agents
        self.proxies = proxies
        self.delay_range = delay_range
        self.scraper = cloudscraper.create_scraper(browser={
            'browser': 'chrome',
            'platform': 'windows',
            'mobile': False
        })
        
    def get_random_user_agent(self):
        return random.choice(self.user_agents)
    
    def get_random_proxy(self):
        if not self.proxies:
            return None
        return random.choice(self.proxies)
    
    def get_cache_filename(self, username):
        return f"cache/{username}.json"
    
    def load_from_cache(self, username):
        cache_file = self.get_cache_filename(username)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r') as f:
                    data = json.load(f)
                logger.info(f"Loaded {username} from cache")
                # Check if data is not too old (7 days)
                if 'timestamp' in data:
                    cache_time = datetime.fromisoformat(data['timestamp'])
                    if datetime.now() - cache_time < timedelta(days=7):
                        return data
            except Exception as e:
                logger.error(f"Error loading cache for {username}: {e}")
        return None
    
    def save_to_cache(self, username, data):
        cache_file = self.get_cache_filename(username)
        try:
            data['timestamp'] = datetime.now().isoformat()
            with open(cache_file, 'w') as f:
                json.dump(data, f)
            logger.info(f"Saved {username} to cache")
        except Exception as e:
            logger.error(f"Error saving cache for {username}: {e}")
    
    def get_page_content(self, url, use_cache=True):
        # Extract username from URL
        username = url.split('/')[-1]
        
        # Check cache first if enabled
        if use_cache:
            cached_data = self.load_from_cache(username)
            if cached_data:
                return cached_data
        
        # Introduce random delay to avoid rate limiting
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)
        
        headers = {
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://www.nairaland.com/',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        proxy = self.get_random_proxy()
        proxy_dict = {"http": proxy, "https": proxy} if proxy else None
        
        try:
            response = self.scraper.get(url, headers=headers, proxies=proxy_dict, timeout=30)
            if response.status_code == 200:
                # Parse the HTML content
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Extract user profile data
                user_data = self.extract_user_data(soup, username)
                
                # Extract user posts
                posts_data = self.extract_posts_data(soup, username)
                
                # Combine data
                result = {
                    "user_info": user_data,
                    "posts": posts_data,
                    "raw_html": response.text  # Store raw HTML for backup analysis
                }
                
                # Save to cache
                self.save_to_cache(username, result)
                
                return result
            else:
                logger.error(f"Failed to get content from {url}: Status code {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"Error getting content from {url}: {e}")
            return None
    
    def extract_user_data(self, soup, username):
        user_data = {
            "username": username,
            "registration_date": None,
            "last_seen": None,
            "signature": None,
            "profile_views": None,
            "follower_count": None,
            "following_count": None,
            "total_posts": None,
        }
        
        try:
            # Look for profile information in tables
            profile_tables = soup.find_all('table', class_='user-profile-table')
            if profile_tables:
                for table in profile_tables:
                    rows = table.find_all('tr')
                    for row in rows:
                        cells = row.find_all('td')
                        if len(cells) >= 2:
                            key = cells[0].get_text(strip=True).lower()
                            value = cells[1].get_text(strip=True)
                            
                            if 'joined' in key or 'registered' in key:
                                user_data['registration_date'] = value
                            elif 'last seen' in key:
                                user_data['last_seen'] = value
                            elif 'posts' in key:
                                user_data['total_posts'] = value
                            elif 'followers' in key:
                                user_data['follower_count'] = value
                            elif 'following' in key:
                                user_data['following_count'] = value
                            elif 'signature' in key:
                                user_data['signature'] = value
                            elif 'profile views' in key:
                                user_data['profile_views'] = value
            
            # If no structured data found, try to use regex on the entire page content
            if user_data['registration_date'] is None:
                page_text = soup.get_text()
                # Common patterns for registration dates
                reg_date_patterns = [
                    r'(?:Joined|Registered):\s*([A-Za-z]+ \d+, \d{4})',
                    r'(?:Joined|Registered):\s*(\d{2}-\d{2}-\d{4})',
                    r'(?:Joined|Registered):\s*(\d{4}-\d{2}-\d{2})'
                ]
                
                for pattern in reg_date_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        user_data['registration_date'] = match.group(1)
                        break
                        
            # Try to find post count if not found yet
            if user_data['total_posts'] is None:
                post_count_pattern = r'Posts:\s*(\d+)'
                match = re.search(post_count_pattern, soup.get_text())
                if match:
                    user_data['total_posts'] = match.group(1)
        
        except Exception as e:
            logger.error(f"Error extracting user data for {username}: {e}")
        
        return user_data
    
    def extract_posts_data(self, soup, username):
        posts = []
        
        try:
            # Find all post containers
            post_containers = soup.find_all('div', class_='post')
            if not post_containers:
                # Try alternative selectors if the expected class isn't found
                post_containers = soup.find_all('table', class_='post-table')
            
            for container in post_containers:
                post_data = {
                    "author": username,
                    "post_date": None,
                    "post_time": None,
                    "post_content": None,
                    "topic_title": None,
                    "is_reply": False,
                    "quoted_user": None,
                    "section": None,
                    "post_id": None,
                    "likes": None,
                    "shares": None
                }
                
                # Try to extract post date and time
                date_element = container.find('div', class_='post-date') or container.find('span', class_='post-date')
                if date_element:
                    date_text = date_element.get_text(strip=True)
                    # Try to split date and time
                    date_parts = re.search(r'([A-Za-z]+ \d+, \d{4})(?: at )?(\d{1,2}:\d{2} [AP]M)?', date_text)
                    if date_parts:
                        post_data['post_date'] = date_parts.group(1)
                        post_data['post_time'] = date_parts.group(2)
                
                # Extract post content
                content_element = container.find('div', class_='post-content') or container.find('td', class_='post-body')
                if content_element:
                    post_data['post_content'] = content_element.get_text(strip=True)
                    
                    # Check if this is a reply (has quoted text)
                    quote_block = content_element.find('blockquote') or content_element.find('div', class_='quoted-post')
                    if quote_block:
                        post_data['is_reply'] = True
                        # Try to extract quoted username
                        quote_header = quote_block.find('div', class_='quote-header') or quote_block.find('strong')
                        if quote_header:
                            quoted_user_match = re.search(r'([A-Za-z0-9_]+)(?:\s+said|:)', quote_header.get_text())
                            if quoted_user_match:
                                post_data['quoted_user'] = quoted_user_match.group(1)
                
                # Try to extract topic title
                topic_element = container.find_previous('h2') or container.find_previous('div', class_='topic-title')
                if topic_element:
                    post_data['topic_title'] = topic_element.get_text(strip=True)
                
                # Try to extract section/category
                section_element = container.find_previous('div', class_='board') or container.find_previous('span', class_='section')
                if section_element:
                    post_data['section'] = section_element.get_text(strip=True)
                
                # Try to extract post ID
                if 'id' in container.attrs:
                    post_data['post_id'] = container['id']
                else:
                    # Try to find ID in a permalink
                    permalink = container.find('a', class_='permalink') or container.find('a', text='Link')
                    if permalink and 'href' in permalink.attrs:
                        post_id_match = re.search(r'#(\d+)', permalink['href'])
                        if post_id_match:
                            post_data['post_id'] = post_id_match.group(1)
                
                # Try to extract likes/shares
                likes_element = container.find('span', class_='like-count') or container.find('a', text=re.compile(r'Likes'))
                if likes_element:
                    likes_match = re.search(r'(\d+)', likes_element.get_text())
                    if likes_match:
                        post_data['likes'] = likes_match.group(1)
                
                shares_element = container.find('span', class_='share-count') or container.find('a', text=re.compile(r'Shares'))
                if shares_element:
                    shares_match = re.search(r'(\d+)', shares_element.get_text())
                    if shares_match:
                        post_data['shares'] = shares_match.group(1)
                
                posts.append(post_data)
        
        except Exception as e:
            logger.error(f"Error extracting posts for {username}: {e}")
        
        return posts

class CoordinationAnalyzer:
    def __init__(self, user_data_dict):
        self.user_data = user_data_dict
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 3),
            min_df=2
        )
        
    def preprocess_posts(self):
        """Preprocess posts for all users"""
        all_posts = []
        user_posts_dict = {}
        
        for username, user_data in self.user_data.items():
            user_posts = []
            for post in user_data.get('posts', []):
                if post.get('post_content'):
                    user_posts.append(post['post_content'])
            
            user_posts_dict[username] = user_posts
            all_posts.extend(user_posts)
        
        return user_posts_dict, all_posts
    
    def calculate_timing_patterns(self):
        """Calculate time patterns of posts for each user"""
        user_timing_patterns = {}
        
        for username, user_data in self.user_data.items():
            post_hours = []
            post_days = []
            post_dates = []
            
            for post in user_data.get('posts', []):
                if post.get('post_time'):
                    try:
                        # Convert time string to hour
                        time_str = post['post_time']
                        if time_str:
                            # Extract hour as int (0-23)
                            hour_match = re.search(r'(\d{1,2}):(\d{2}) ([AP]M)', time_str)
                            if hour_match:
                                hour = int(hour_match.group(1))
                                minute = int(hour_match.group(2))
                                am_pm = hour_match.group(3)
                                
                                if am_pm.upper() == 'PM' and hour < 12:
                                    hour += 12
                                elif am_pm.upper() == 'AM' and hour == 12:
                                    hour = 0
                                    
                                post_hours.append(hour)
                    except:
                        pass
                        
                if post.get('post_date'):
                    try:
                        # Extract day of week from date
                        date_str = post['post_date']
                        post_date = None
                        
                        # Handle different date formats
                        date_formats = [
                            '%B %d, %Y',  # e.g., January 1, 2023
                            '%d-%m-%Y',   # e.g., 01-01-2023
                            '%Y-%m-%d'    # e.g., 2023-01-01
                        ]
                        
                        for fmt in date_formats:
                            try:
                                post_date = datetime.strptime(date_str, fmt)
                                break
                            except:
                                continue
                                
                        if post_date:
                            post_days.append(post_date.strftime('%A'))  # Day name (Monday, Tuesday, etc.)
                            post_dates.append(post_date)
                    except:
                        pass
            
            user_timing_patterns[username] = {
                'hours': post_hours,
                'days': post_days,
                'dates': post_dates
            }
            
        return user_timing_patterns
    
    def calculate_content_similarity(self):
        """Calculate content similarity between users based on their posts"""
        user_posts_dict, all_posts = self.preprocess_posts()
        
        if not all_posts:
            return {}, {}, {}
        
        # Create a combined text for each user
        user_combined_texts = {}
        for username, posts in user_posts_dict.items():
            if posts:
                user_combined_texts[username] = ' '.join(posts)
            else:
                user_combined_texts[username] = ""
        
        # Get list of usernames with non-empty posts
        valid_usernames = [u for u, t in user_combined_texts.items() if t.strip()]
        
        if len(valid_usernames) < 2:
            return {}, {}, {}
        
        # Create corpus of texts
        corpus = [user_combined_texts[u] for u in valid_usernames]
        
        # Fit TF-IDF vectorizer
        try:
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
            
            # Calculate cosine similarity
            cosine_sim = cosine_similarity(tfidf_matrix)
            
            # Create similarity matrix
            similarity_matrix = {}
            shared_topics = {}
            shared_phrases = {}
            
            # Get the feature names (terms) from the vectorizer
            try:
                feature_names = self.tfidf_vectorizer.get_feature_names_out()
            except:
                feature_names = self.tfidf_vectorizer.get_feature_names()
            
            # For each pair of users
            for i, user1 in enumerate(valid_usernames):
                similarity_matrix[user1] = {}
                shared_topics[user1] = {}
                shared_phrases[user1] = {}
                
                for j, user2 in enumerate(valid_usernames):
                    if i != j:  # Don't compare user with themselves
                        similarity_matrix[user1][user2] = cosine_sim[i, j]
                        
                        # Identify common topics/phrases
                        if cosine_sim[i, j] > 0.3:  # Threshold for significant similarity
                            # Get the vectors for both users
                            vec1 = tfidf_matrix[i].toarray().flatten()
                            vec2 = tfidf_matrix[j].toarray().flatten()
                            
                            # Find terms with high TF-IDF scores in both users
                            common_terms_indices = []
                            for idx, (score1, score2) in enumerate(zip(vec1, vec2)):
                                if score1 > 0.1 and score2 > 0.1:  # Both users use this term significantly
                                    common_terms_indices.append(idx)
                            
                            # Get the actual terms
                            common_terms = [feature_names[idx] for idx in common_terms_indices]
                            
                            # Group them into phrases (n-grams with n > 1)
                            phrases = [term for term in common_terms if ' ' in term]
                            topics = [term for term in common_terms if ' ' not in term]
                            
                            shared_topics[user1][user2] = topics
                            shared_phrases[user1][user2] = phrases
            
            return similarity_matrix, shared_topics, shared_phrases
        except Exception as e:
            logger.error(f"Error calculating content similarity: {e}")
            return {}, {}, {}
    
    def analyze_coordination(self):
        """Perform comprehensive coordination analysis"""
        # Get timing patterns
        timing_patterns = self.calculate_timing_patterns()
        
        # Get content similarity
        similarity_matrix, shared_topics, shared_phrases = self.calculate_content_similarity()
        
        # Calculate temporal coordination scores
        temporal_coordination = self.calculate_temporal_coordination(timing_patterns)
        
        # Calculate reply network
        reply_network = self.calculate_reply_network()
        
        # Calculate response time patterns
        response_times = self.calculate_response_times(reply_network)
        
        # Calculate registration date proximity
        reg_date_proximity = self.calculate_registration_proximity()
        
        # Calculate post volume correlation
        post_volume_correlation = self.calculate_post_volume_correlation(timing_patterns)
        
        # Calculate final coordination scores
        coordination_scores = self.calculate_final_coordination_scores(
            similarity_matrix,
            temporal_coordination,
            reply_network,
            reg_date_proximity,
            post_volume_correlation
        )
        
        return {
            'similarity_matrix': similarity_matrix,
            'shared_topics': shared_topics,
            'shared_phrases': shared_phrases,
            'timing_patterns': timing_patterns,
            'temporal_coordination': temporal_coordination,
            'reply_network': reply_network,
            'response_times': response_times,
            'reg_date_proximity': reg_date_proximity,
            'post_volume_correlation': post_volume_correlation,
            'coordination_scores': coordination_scores
        }
    
    def calculate_temporal_coordination(self, timing_patterns):
        """Calculate temporal coordination between user pairs"""
        temporal_scores = {}
        
        for user1, pattern1 in timing_patterns.items():
            temporal_scores[user1] = {}
            
            for user2, pattern2 in timing_patterns.items():
                if user1 != user2:
                    hour_score = 0
                    day_score = 0
                    
                    # Calculate hour overlap score
                    if pattern1['hours'] and pattern2['hours']:
                        # Convert to hour frequency distributions
                        hours1 = Counter(pattern1['hours'])
                        hours2 = Counter(pattern2['hours'])
                        
                        # Normalize
                        total1 = sum(hours1.values())
                        total2 = sum(hours2.values())
                        
                        if total1 > 0 and total2 > 0:
                            normalized_hours1 = {h: c/total1 for h, c in hours1.items()}
                            normalized_hours2 = {h: c/total2 for h, c in hours2.items()}
                            
                            # Calculate overlap for each hour
                            overlap_sum = 0
                            for hour in range(24):
                                h1_freq = normalized_hours1.get(hour, 0)
                                h2_freq = normalized_hours2.get(hour, 0)
                                overlap_sum += min(h1_freq, h2_freq)
                            
                            hour_score = overlap_sum
                    
                    # Calculate day overlap score
                    if pattern1['days'] and pattern2['days']:
                        days1 = Counter(pattern1['days'])
                        days2 = Counter(pattern2['days'])
                        
                        total1 = sum(days1.values())
                        total2 = sum(days2.values())
                        
                        if total1 > 0 and total2 > 0:
                            normalized_days1 = {d: c/total1 for d, c in days1.items()}
                            normalized_days2 = {d: c/total2 for d, c in days2.items()}
                            
                            # Calculate overlap for each day
                            days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                            overlap_sum = 0
                            for day in days_of_week:
                                d1_freq = normalized_days1.get(day, 0)
                                d2_freq = normalized_days2.get(day, 0)
                                overlap_sum += min(d1_freq, d2_freq)
                            
                            day_score = overlap_sum
                    
                    # Combined temporal score (weight hour patterns more heavily)
                    temporal_scores[user1][user2] = (0.7 * hour_score) + (0.3 * day_score)
        
        return temporal_scores
    
    def calculate_reply_network(self):
        """Calculate reply network between users"""
        reply_counts = {}
        
        for username, user_data in self.user_data.items():
            reply_counts[username] = {}
            
            for post in user_data.get('posts', []):
                if post.get('is_reply') and post.get('quoted_user'):
                    quoted_user = post['quoted_user']
                    if quoted_user in self.user_data:  # Only count replies to users in our dataset
                        reply_counts[username][quoted_user] = reply_counts[username].get(quoted_user, 0) + 1
        
        return reply_counts
    
    def calculate_response_times(self, reply_network):
        """Calculate how quickly users respond to each other"""
        response_times = {}
        
        for user1, user_data1 in self.user_data.items():
            response_times[user1] = {}
            
            # Sort user1's posts by date+time
            sorted_posts1 = []
            for post in user_data1.get('posts', []):
                try:
                    if post.get('post_date') and post.get('post_time'):
                        date_str = post['post_date']
                        time_str = post['post_time']
                        
                        # Try different date formats
                        date_formats = ['%B %d, %Y', '%d-%m-%Y', '%Y-%m-%d']
                        parsed_date = None
                        
                        for fmt in date_formats:
                            try:
                                parsed_date = datetime.strptime(date_str, fmt)
                                break
                            except:
                                continue
                        
                        if parsed_date and time_str:
                            # Extract time components
                            time_match = re.search(r'(\d{1,2}):(\d{2}) ([AP]M)', time_str)
                            if time_match:
                                hour = int(time_match.group(1))
                                minute = int(time_match.group(2))
                                am_pm = time_match.group(3)
                                
                                if am_pm.upper() == 'PM' and hour < 12:
                                    hour += 12
                                elif am_pm.upper() == 'AM' and hour == 12:
                                    hour = 0
                                
                                # Create full datetime
                                post_datetime = parsed_date.replace(hour=hour, minute=minute)
                                
                                sorted_posts1.append({
                                    'datetime': post_datetime,
                                    'is_reply': post.get('is_reply', False),
                                    'quoted_user': post.get('quoted_user'),
                                    'post_id': post.get('post_id')
                                })
                except:
                    pass
            
            # Sort by datetime
            sorted_posts1.sort(key=lambda x: x['datetime'])
            
            # For each user that user1 replies to
            for user2 in reply_network.get(user1, {}):
                if user2 != user1:
                    response_delays = []
                    
                    # Get user2's posts
                    user_data2 = self.user_data.get(user2, {})
                    sorted_posts2 = []
                    
                    for post in user_data2.get('posts', []):
                        try:
                            if post.get('post_date') and post.get('post_time'):
                                date_str = post['post_date']
                                time_str = post['post_time']
                                
                                # Try different date formats
                                date_formats = ['%B %d, %Y', '%d-%m-%Y', '%Y-%m-%d']
                                parsed_date = None
                                
                                for fmt in date_formats:
                                    try:
                                        parsed_date = datetime.strptime(date_str, fmt)
                                        break
                                    except:
                                        continue
                                
                                if parsed_date and time_str:
                                    # Extract time components
                                    time_match = re.search(r'(\d{1,2}):(\d{2}) ([AP]M)', time_str)
                                    if time_match:
                                        hour = int(time_match.group(1))
                                        minute = int(time_match.group(2))
                                        am_pm = time_match.group(3)
                                        
                                        if am_pm.upper() == 'PM' and hour < 12:
                                            hour += 12
                                        elif am_pm.upper() == 'AM' and hour == 12:
                                            hour = 0
                                        
                                        # Create full datetime
                                        post_datetime = parsed_date.replace(hour=hour, minute=minute)
                                        
                                        sorted_posts2.append({
                                            'datetime': post_datetime,
                                            'post_id': post.get('post_id')
                                        })
                        except:
                            pass
                    
                    # Sort user2's posts by datetime
                    sorted_posts2.sort(key=lambda x: x['datetime'])
                    
                    # For each of user1's posts that is a reply to user2
                    for post1 in sorted_posts1:
                        if post1['is_reply'] and post1['quoted_user'] == user2:
                            # Find the most recent post from user2 before user1's reply
                            prev_post = None
                            for post2 in sorted_posts2:
                                if post2['datetime'] < post1['datetime']:
                                    prev_post = post2
                                else:
                                    break
                            
                            if prev_post:
                                # Calculate time difference in minutes
                                time_diff = (post1['datetime'] - prev_post['datetime']).total_seconds() / 60
                                response_delays.append(time_diff)
                    
                    # Calculate average response time if we have any valid measurements
                    if response_delays:
                        response_times[user1][user2] = {
                            'mean': np.mean(response_delays),
                            'median': np.median(response_delays),
                            'min': min(response_delays),
                            'max': max(response_delays),
                            'count': len(response_delays)
                        }
                    else:
                        response_times[user1][user2] = {
                            'mean': None,
                            'median': None,
                            'min': None,
                            'max': None,
                            'count': 0
                        }
        
        return response_times
    
    def calculate_registration_proximity(self):
        """Calculate how close users registered to each other"""
        reg_proximity = {}
        
        # Extract registration dates for each user
        reg_dates = {}
        for username, user_data in self.user_data.items():
            reg_date = user_data.get('user_info', {}).get('registration_date')
            if reg_date:
                # Try to parse the registration date
                date_formats = ['%B %d, %Y', '%d-%m-%Y', '%Y-%m-%d']
                parsed_date = None
                
                for fmt in date_formats:
                    try:
                        parsed_date = datetime.strptime(reg_date, fmt)
                        break
                    except:
                        continue
                
                if parsed_date:
                    reg_dates[username] = parsed_date
        
        # Calculate proximity for each pair of users
        for user1, date1 in reg_dates.items():
            reg_proximity[user1] = {}
            
            for user2, date2 in reg_dates.items():
                if user1 != user2:
                    # Calculate days between registrations
                    days_diff = abs((date1 - date2).days)
                    
                    # Convert to a proximity score (closer = higher score)
                    # Score ranges from 0 to 1, where 1 means same day registration
                    # and approaches 0 as the time gap increases
                    if days_diff == 0:
                        proximity = 1.0
                    else:
                        proximity = 1.0 / (1.0 + (days_diff / 7.0))  # Decay function
                    
                    reg_proximity[user1][user2] = {
                        'days_diff': days_diff,
                        'proximity_score': proximity
                    }
        
        return reg_proximity
    
    def calculate_post_volume_correlation(self, timing_patterns):
        """Calculate correlation between users' posting volume over time"""
        volume_correlation = {}
        
        # Get users with date information
        users_with_dates = {}
        for username, patterns in timing_patterns.items():
            if patterns['dates']:
                users_with_dates[username] = patterns['dates']
        
        if len(users_with_dates) < 2:
            return {}
        
        # Find the overall date range
        all_dates = []
        for username, dates in users_with_dates.items():
            all_dates.extend(dates)
        
        if not all_dates:
            return {}
        
        min_date = min(all_dates)
        max_date = max(all_dates)
        
        # Create date bins (weekly)
        date_range = (max_date - min_date).days
        num_bins = max(1, date_range // 7)
        
        # Create a time series of post counts for each user
        user_time_series = {}
        for username, dates in users_with_dates.items():
            # Initialize bins
            bins = [0] * (num_bins + 1)
            
            # Count posts in each bin
            for post_date in dates:
                bin_index = min(num_bins, (post_date - min_date).days // 7)
                bins[bin_index] += 1
            
            user_time_series[username] = bins
        
        # Calculate correlation between each pair of users
        for user1, series1 in user_time_series.items():
            volume_correlation[user1] = {}
            
            for user2, series2 in user_time_series.items():
                if user1 != user2:
                    # Calculate correlation coefficient if there's enough data
                    if sum(series1) > 0 and sum(series2) > 0 and len(series1) > 1:
                        try:
                            correlation = np.corrcoef(series1, series2)[0, 1]
                            volume_correlation[user1][user2] = correlation
                        except:
                            volume_correlation[user1][user2] = 0
                    else:
                        volume_correlation[user1][user2] = 0
        
        return volume_correlation
    
    def calculate_final_coordination_scores(self, similarity_matrix, temporal_coordination, 
                                       reply_network, reg_date_proximity, post_volume_correlation):
        """Calculate final coordination scores between user pairs"""
        coordination_scores = {}
        
        # Weights for different factors
        weights = {
            'content_similarity': 0.35,
            'temporal_coordination': 0.25,
            'reply_frequency': 0.15,
            'reg_proximity': 0.15,
            'volume_correlation': 0.10
        }
        
        # Normalize reply_network to get reply frequency scores
        reply_freq_scores = {}
        max_replies = 1  # To avoid division by zero
        
        for user1, replies in reply_network.items():
            for user2, count in replies.items():
                max_replies = max(max_replies, count)
        
        for user1, replies in reply_network.items():
            reply_freq_scores[user1] = {}
            for user2, count in replies.items():
                reply_freq_scores[user1][user2] = count / max_replies
        
        # Calculate final scores
        all_users = set()
        for matrix in [similarity_matrix, temporal_coordination, reply_freq_scores]:
            for user in matrix:
                all_users.add(user)
                for user2 in matrix[user]:
                    all_users.add(user2)
        
        for user1 in all_users:
            coordination_scores[user1] = {}
            
            for user2 in all_users:
                if user1 != user2:
                    # Initialize scores
                    scores = {
                        'content_similarity': 0,
                        'temporal_coordination': 0,
                        'reply_frequency': 0,
                        'reg_proximity': 0,
                        'volume_correlation': 0
                    }
                    
                    # Assign scores from each factor
                    if user1 in similarity_matrix and user2 in similarity_matrix[user1]:
                        scores['content_similarity'] = similarity_matrix[user1][user2]
                    
                    if user1 in temporal_coordination and user2 in temporal_coordination[user1]:
                        scores['temporal_coordination'] = temporal_coordination[user1][user2]
                    
                    if user1 in reply_freq_scores and user2 in reply_freq_scores[user1]:
                        scores['reply_frequency'] = reply_freq_scores[user1][user2]
                    
                    if (user1 in reg_date_proximity and user2 in reg_date_proximity[user1] and 
                        'proximity_score' in reg_date_proximity[user1][user2]):
                        scores['reg_proximity'] = reg_date_proximity[user1][user2]['proximity_score']
                    
                    if user1 in post_volume_correlation and user2 in post_volume_correlation[user1]:
                        # Convert correlation (-1 to 1) to a 0-1 score
                        correlation = post_volume_correlation[user1][user2]
                        scores['volume_correlation'] = (correlation + 1) / 2
                    
                    # Calculate weighted average
                    weighted_score = sum(weights[factor] * score for factor, score in scores.items())
                    
                    coordination_scores[user1][user2] = {
                        'overall_score': weighted_score,
                        'component_scores': scores
                    }
        
        return coordination_scores
